import pandas as pd

df = pd.read_csv("processed_data/train_combined_sep.csv")
df_test = pd.read_csv("processed_data/test_cleaned_sep.csv")


from sklearn.preprocessing import LabelEncoder
import torch
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler

# 1. ë ˆì´ë¸” ì¸ì½”ë” ìƒì„± ë° í•™ìŠµ
le = LabelEncoder()
df['label'] = le.fit_transform(df['class'])
# í´ë˜ìŠ¤-ìˆ«ì ë§¤í•‘ ê²°ê³¼ ì €ì¥ (ë‚˜ì¤‘ì— ì¶”ë¡  ì‹œ ê²°ê³¼ í•´ì„ì„ ìœ„í•´ í•„ìš”)
mapping = dict(zip(range(len(le.classes_)), le.classes_))
print("ğŸ“Œ ë ˆì´ë¸” ë§¤í•‘ ê²°ê³¼:", mapping)

# 2. íŒŒì´í† ì¹˜ í…ì„œë¡œ ë³€í™˜
# ì•ì„œ ë§Œë“  input_idsì™€ attention_masksë¥¼ í•¨ê»˜ ì‚¬ìš©í•©ë‹ˆë‹¤.
train_labels = torch.tensor(df['label'].values)
print(f"ë ˆì´ë¸” í…ì„œ ìƒì„± ì™„ë£Œ: {train_labels.shape}")

from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding

# 1. í™˜ê²½ ì„¤ì •
CHECKPOINT_NAME = "klue/bert-base"
tokenizer_pretrained = CHECKPOINT_NAME

# 2. ë°ì´í„° ë¶„í•  (Stratify ì ìš©)
# í•™ìŠµ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„°ë¥¼ 8:2 ë¹„ìœ¨ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.
train_df, val_df = train_test_split(
    df,
    test_size=0.2,
    random_state=42,
    stratify=df['label']  # í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€
)

# 3. ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ (ì œê³µí•´ì£¼ì‹  ì½”ë“œ ìˆ˜ì •)
class TokenDataset(Dataset):
    def __init__(self, dataframe, tokenizer_pretrained):
        self.data = dataframe
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_pretrained)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        # ì»¬ëŸ¼ëª…ì— ë§ì¶° ìˆ˜ì •: 'document' -> 'conversation'
        sentence = self.data.iloc[idx]['conversation']
        label = self.data.iloc[idx]['label']

        tokens = self.tokenizer(
            str(sentence),           # ë¬¸ì¥ ì „ë‹¬
            return_tensors='pt',
            truncation=True,
            padding='max_length',
            max_length=512,          # ê¸¸ì´ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •
            add_special_tokens=True
        )

        input_ids = tokens['input_ids'].squeeze(0)
        attention_mask = tokens['attention_mask'].squeeze(0)
        token_type_ids = torch.zeros_like(attention_mask)

        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'token_type_ids': token_type_ids,
        }, torch.tensor(label, dtype=torch.long)

# 4. Dataset ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
train_dataset = TokenDataset(train_df, tokenizer_pretrained)
val_dataset = TokenDataset(val_df, tokenizer_pretrained)

# 5. DataLoader ì„¤ì •
train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=8)
val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=8)

print(f"âœ… DataLoader êµ¬ì¶• ì™„ë£Œ (Stratify ì ìš©)")
print(f"í•™ìŠµ ë°ì´í„° ê°œìˆ˜: {len(train_df)} | ê²€ì¦ ë°ì´í„° ê°œìˆ˜: {len(val_df)}")

import torch

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}')

import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from transformers import BertModel


class CustomBertModel(nn.Module):
    def __init__(self, bert_pretrained, dropout_rate=0.5):
        super(CustomBertModel, self).__init__()
        # ì‚¬ì „í•™ìŠµ ëª¨ë¸ ì§€ì •
        self.bert = BertModel.from_pretrained(bert_pretrained)
        self.dr = nn.Dropout(p=dropout_rate)
        # 5 class ë¶„ë¥˜
        self.fc = nn.Linear(768, 5)

    def forward(self, input_ids, attention_mask, token_type_ids):
        output = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)
        last_hidden_state = output['last_hidden_state']
        # last_hidden_state[:, 0, :]ëŠ” [CLS] í† í°ì„ ê°€ì ¸ì˜´
        x = self.dr(last_hidden_state[:, 0, :])
        # FC ì„ ê±°ì³ ìµœì¢… ì¶œë ¥
        x = self.fc(x)
        return x

bert = CustomBertModel(CHECKPOINT_NAME)
bert.to(device)

loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(bert.parameters(), lr=1e-5)

import torch
import numpy as np
from tqdm import tqdm
from sklearn.metrics import f1_score

# í´ë˜ìŠ¤ ì´ë¦„ ì •ì˜ (íŒŒì¼ í—¤ë” ë° ì¶œë ¥ìš©)
target_names = ['í˜‘ë°•', 'ê°ˆì·¨', 'ì§ì¥ ë‚´ ê´´ë¡­í˜', 'ê¸°íƒ€ ê´´ë¡­í˜', 'ì¼ë°˜ ëŒ€í™”']

def model_train(model, data_loader, loss_fn, optimizer, device):
    model.train()
    running_loss = 0
    corr = 0
    counts = 0

    all_preds = []
    all_labels = []

    progress_bar = tqdm(data_loader, unit='batch', total=len(data_loader), mininterval=1)

    for idx, (inputs, labels) in enumerate(progress_bar):
        inputs = {k: v.to(device) for k, v in inputs.items()}
        labels = labels.to(device)

        optimizer.zero_grad()
        output = model(**inputs)
        logits = output.logits if hasattr(output, 'logits') else output

        loss = loss_fn(logits, labels)
        loss.backward()
        optimizer.step()

        _, pred = logits.max(dim=1)

        corr += pred.eq(labels).sum().item()
        counts += len(labels)
        running_loss += loss.item() * labels.size(0)

        all_preds.extend(pred.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

        # ì§„í–‰ ë°” ì—…ë°ì´íŠ¸
        current_f1 = f1_score(all_labels, all_preds, average='weighted')
        progress_bar.set_description(f"Loss: {running_loss/counts:.4f}, Acc: {corr/counts:.4f}, F1: {current_f1:.4f}")

    final_loss = running_loss / len(data_loader.dataset)
    final_acc = corr / len(data_loader.dataset)
    # ğŸ”¹ í´ë˜ìŠ¤ë³„ F1 Score ê³„ì‚° (average=None)
    final_f1_per_class = f1_score(all_labels, all_preds, average=None)
    final_f1_weighted = f1_score(all_labels, all_preds, average='weighted')

    return final_loss, final_acc, final_f1_weighted, final_f1_per_class

def model_evaluate(model, data_loader, loss_fn, device):
    model.eval()
    running_loss = 0
    corr = 0
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in data_loader:
            inputs = {k: v.to(device) for k, v in inputs.items()}
            labels = labels.to(device)

            output = model(**inputs)
            logits = output.logits if hasattr(output, 'logits') else output

            _, pred = logits.max(dim=1)
            corr += torch.sum(pred.eq(labels)).item()
            running_loss += loss_fn(logits, labels).item() * labels.size(0)

            all_preds.extend(pred.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    final_loss = running_loss / len(data_loader.dataset)
    final_acc = corr / len(data_loader.dataset)
    # ğŸ”¹ í´ë˜ìŠ¤ë³„ F1 Score ê³„ì‚°
    final_f1_per_class = f1_score(all_labels, all_preds, average=None)
    final_f1_weighted = f1_score(all_labels, all_preds, average='weighted')

    return final_loss, final_acc, final_f1_weighted, final_f1_per_class

# --- ì‹¤í–‰ë¶€ ---
num_epochs = 20
model_name = 'bert-kor-base'
min_loss = np.inf
metric_path = f"{model_name}_metrics.txt"

# ğŸ”¹ íŒŒì¼ í—¤ë” ìˆ˜ì • (í´ë˜ìŠ¤ë³„ F1 ì»¬ëŸ¼ ì¶”ê°€)
f1_headers = "\t".join([f"T_F1_{name}" for name in target_names] + [f"V_F1_{name}" for name in target_names])
with open(metric_path, 'w') as f:
    f.write(f"Epoch\tTrain_Loss\tTrain_Acc\tTrain_F1_W\tVal_Loss\tVal_Acc\tVal_F1_W\t{f1_headers}\n")

for epoch in range(num_epochs):
    # Training
    train_loss, train_acc, train_f1_w, train_f1_class = model_train(
        bert, train_loader, loss_fn, optimizer, device
    )

    # Evaluation
    val_loss, val_acc, val_f1_w, val_f1_class = model_evaluate(
        bert, val_loader, loss_fn, device
    )

    # Checkpoint (Validation Loss ê¸°ì¤€)
    if val_loss < min_loss:
        print(f'âœ¨ [INFO] val_loss improved to {val_loss:.5f}. Saving Model!')
        min_loss = val_loss
        torch.save(bert.state_dict(), f'/home/summer24/DataFrom101/ddd/DKTC_classification/íƒœê²½/result/{model_name}.pth')

    # ì½˜ì†” ì¶œë ¥ (ì „ì²´ ìš”ì•½)
    print(f'Epoch [{epoch+1:02d}/{num_epochs}]')
    print(f'TRAIN | Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | F1(W): {train_f1_w:.4f}')
    print(f'VALID | Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | F1(W): {val_f1_w:.4f}')
    
    # í´ë˜ìŠ¤ë³„ F1 ì ìˆ˜ ìƒì„¸ ì¶œë ¥
    print("Class F1 (Val): " + " | ".join([f"{name}: {score:.4f}" for name, score in zip(target_names, val_f1_class)]))
    print('-' * 50)

    # ğŸ”¹ TXT íŒŒì¼ì— ì €ì¥
    train_f1_str = "\t".join([f"{s:.4f}" for s in train_f1_class])
    val_f1_str = "\t".join([f"{s:.4f}" for s in val_f1_class])
    
    with open(metric_path, 'a') as f:
        f.write(
            f"{epoch+1}\t"
            f"{train_loss:.4f}\t{train_acc:.4f}\t{train_f1_w:.4f}\t"
            f"{val_loss:.4f}\t{val_acc:.4f}\t{val_f1_w:.4f}\t"
            f"{train_f1_str}\t{val_f1_str}\n"
        )