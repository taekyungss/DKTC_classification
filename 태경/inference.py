import json
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, BertModel
from tqdm import tqdm

# --- 1. ëª¨ë¸ êµ¬ì¡° ì •ì˜ (ë™ì¼ ìœ ì§€) ---
class CustomBertSupConModel(nn.Module):
    def __init__(self, bert_pretrained, dropout_rate=0.5):
        super(CustomBertSupConModel, self).__init__()
        self.bert = BertModel.from_pretrained(bert_pretrained)
        self.dr = nn.Dropout(p=dropout_rate)
        self.fc = nn.Linear(768, 5)
        self.projection_head = nn.Sequential(
            nn.Linear(768, 768),
            nn.ReLU(),
            nn.Linear(768, 128)
        )

    def forward(self, input_ids, attention_mask, token_type_ids):
        output = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)
        cls_token = output['last_hidden_state'][:, 0, :]
        logits = self.fc(self.dr(cls_token))
        proj_feat = F.normalize(self.projection_head(cls_token), dim=1)
        return logits, proj_feat

# --- 2. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì •ì˜ (ë™ì¼ ìœ ì§€) ---
class TestTokenDataset(Dataset):
    def __init__(self, texts, tokenizer_name):
        self.texts = texts
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)

    def __len__(self): return len(self.texts)

    def __getitem__(self, idx):
        sentence = self.texts[idx]
        tokens = self.tokenizer(
            str(sentence), return_tensors='pt', truncation=True,
            padding='max_length', max_length=512, add_special_tokens=True
        )
        return {
            'input_ids': tokens['input_ids'].squeeze(0),
            'attention_mask': tokens['attention_mask'].squeeze(0),
            'token_type_ids': torch.zeros(512, dtype=torch.long),
        }

# --- 3. ë©”ì¸ ì¶”ë¡  í•¨ìˆ˜ ---
def run_inference_to_index():
    CHECKPOINT_NAME = "monologg/kobigbird-bert-base"
    MODEL_PATH = '/home/summer24/DataFrom101/ddd/DKTC_classification/íƒœê²½/best_f1_model.pth'
    TEST_JSON_PATH = 'processed_data/test.json'
    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # 1. ëª¨ë¸ ë¡œë“œ
    print("ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘...")
    model = CustomBertSupConModel(CHECKPOINT_NAME).to(DEVICE)
    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))
    model.eval()

    # 2. ë°ì´í„° ë¡œë“œ
    with open(TEST_JSON_PATH, "r", encoding="utf-8") as f:
        test_json = json.load(f)
    
    test_ids = list(test_json.keys())
    test_texts = [test_json[tid]['text'] for tid in test_ids]
    
    test_loader = DataLoader(TestTokenDataset(test_texts, CHECKPOINT_NAME), batch_size=64, shuffle=False)

    # 3. ì¶”ë¡  (Inference)
    raw_preds = []
    with torch.no_grad():
        for inputs in tqdm(test_loader, desc="Inference"):
            inputs = {k: v.to(DEVICE) for k, v in inputs.items()}
            logits, _ = model(**inputs)
            preds = torch.argmax(logits, dim=1).cpu().numpy()
            raw_preds.extend(preds)

    # 4. ğŸ”¥ ì¸ë±ìŠ¤ ì¬ë§¤í•‘ (ê°€ì¥ ì¤‘ìš”í•œ ë¶€ë¶„)
    # ëª¨ë¸ ì¶œë ¥ê°’(ê°€ë‚˜ë‹¤ìˆœ) -> ìµœì¢… ì œì¶œ ì¸ë±ìŠ¤
    # ëª¨ë¸ 0(ê°ˆì·¨) -> ì œì¶œ 1
    # ëª¨ë¸ 1(ê¸°íƒ€) -> ì œì¶œ 3
    # ëª¨ë¸ 2(ì¼ë°˜) -> ì œì¶œ 4
    # ëª¨ë¸ 3(ì§ì¥) -> ì œì¶œ 2
    # ëª¨ë¸ 4(í˜‘ë°•) -> ì œì¶œ 0
    remapping_dict = {
        0: 1,  # ê°ˆì·¨
        1: 3,  # ê¸°íƒ€ ê´´ë¡­í˜
        2: 4,  # ì¼ë°˜
        3: 2,  # ì§ì¥ ë‚´ ê´´ë¡­í˜
        4: 0   # í˜‘ë°•
    }
    
    final_preds = [remapping_dict[p] for p in raw_preds]

    # 5. ê²°ê³¼ ì €ì¥
    submission_df = pd.DataFrame({
        'file_name': test_ids,
        'class': final_preds
    })

    output_filename = "submission_final_kobigbir.csv"
    submission_df.to_csv(output_filename, index=False, encoding='utf-8')
    
    print("-" * 30)
    print(f"âœ… ì¬ë§¤í•‘ ì™„ë£Œ! ì œì¶œ íŒŒì¼ ìƒì„±ë¨: {output_filename}")
    print(f"ğŸ“Š ë§¤í•‘ ê²°ê³¼ ì˜ˆì‹œ (ì²˜ìŒ 5ê°œ):")
    for i in range(5):
        print(f"ID: {test_ids[i]} | ëª¨ë¸ì¶œë ¥: {raw_preds[i]} -> ìµœì¢…ì¸ë±ìŠ¤: {final_preds[i]}")

if __name__ == "__main__":
    run_inference_to_index()