{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VEcFLvfqc7qT"
   },
   "outputs": [],
   "source": [
    "# !apt-get install -y fonts-nanum\n",
    "# !fc-cache -fv\n",
    "# !rm ~/.cache/matplotlib -rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUeHYIawI2IB"
   },
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zq_cxmv2ZGTA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/DLthon/train (2).csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "se6Va-1TbxVx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rc('font', family='NanumBarunGothic')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(data=df, x='class', order=df['class'].value_counts().index)\n",
    "plt.title('í´ë˜ìŠ¤ë³„ ë°ì´í„° ê°œìˆ˜ ë¶„í¬')\n",
    "plt.xlabel('ë²”ì£„ ìœ í˜•')\n",
    "plt.ylabel('ë°ì´í„° ìˆ˜')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# ìˆ˜ì¹˜ë¡œ í™•ì¸\n",
    "print(df['class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8NiX5LUdfWX"
   },
   "source": [
    "ê·¼ë° ë§Œì•½ ìš°ë¦¬ê°€ ë§Œë“œëŠ” í•©ì„± ë°ì´í„°ë¥¼ ì¶”ê°€í•œë‹¤ë©´ (ì¼ë°˜ ë°ì´í„°) -> ë¶ˆê· í˜• ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ\n",
    "\n",
    "í•™ìŠµì„ ìœ„í•´ì„œëŠ” ì¶©ë¶„í•œ ë°ì´í„°ê°€ í•„ìš”í• ê²ƒ ê°™ì€ë° í•©ì„± ë°ì´í„°ë¥¼ ëª‡ê°œ ë§Œë“¤ë©´ ì¢‹ì„ì§€ ê·¸ë¦¬ê³  ì–´ë– í•œ ë°©ì‹ìœ¼ë¡œ ë§Œë“¤ì–´ì•¼í• ì§€, ë¶ˆê· í˜• ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ ì²˜ë¦¬í•´ì•¼í• ì§€ ë“±ê³¼ ê°™ì€ ë¶€ë¶„ë“±ì„ ì •í•´ì•¼í• ê²ƒ ê°™ìŒ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ey8xf3ERdPcZ"
   },
   "outputs": [],
   "source": [
    "# 1. ê¸€ì ìˆ˜ ê³„ì‚°\n",
    "df['char_len'] = df['conversation'].str.len()\n",
    "\n",
    "# 2. ë‹¨ì–´ ìˆ˜ ê³„ì‚° (ë„ì–´ì“°ê¸° ê¸°ì¤€)\n",
    "df['word_cnt'] = df['conversation'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# ì‹œê°í™”: ê¸€ì ìˆ˜ ë¶„í¬\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['char_len'], bins=30, kde=True, color='blue')\n",
    "plt.title('ì „ì²´ ëŒ€í™” ê¸€ì ìˆ˜ ë¶„í¬')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x='class', y='char_len', data=df)\n",
    "plt.title('í´ë˜ìŠ¤ë³„ ê¸€ì ìˆ˜ ë¶„í¬ (Boxplot)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JQ_xSWudyC3"
   },
   "source": [
    "ì „ì²´ ëŒ€í™”ì˜ ê¸€ìëŠ” 150~250ì ì‚¬ì´ ê°€ ê°€ì¥ ë§ìŒ\n",
    "ê·¸ëŸ¬ë‚˜ ëª‡ê°œëŠ” 800ìê°€ ë„˜ì–´ê°€ëŠ” ë§¤ìš° ê¸´ ëŒ€í™”ë„ ë³´ì„\n",
    "\n",
    "ëª¨ë¸ë§ í• ë•Œ 300-400 ì •ë„ì˜ max_seq_len ê³ ë¯¼í•´ë´ì•¼í• ë“¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Of5mxv5dRfE"
   },
   "outputs": [],
   "source": [
    "stats = df.groupby('class')['char_len'].agg(['mean', 'median', 'std', 'min', 'max'])\n",
    "print(\"--- í´ë˜ìŠ¤ë³„ ê¸€ì ìˆ˜ í†µê³„ ìš”ì•½ ---\")\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W7T-qlTfdTk_"
   },
   "outputs": [],
   "source": [
    "# 1. 600ì ì´ˆê³¼ ë°ì´í„°ë§Œ ì¶”ì¶œ\n",
    "long_conv_df = df[df['char_len'] > 600]\n",
    "\n",
    "print(f\"600ì ì´ˆê³¼ ë°ì´í„° ê°œìˆ˜: {len(long_conv_df)}ê°œ\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 2. í´ë˜ìŠ¤ë³„ë¡œ ëª‡ ê°œì”© ìˆëŠ”ì§€ í™•ì¸\n",
    "print(\"í´ë˜ìŠ¤ë³„ ë¶„í¬:\")\n",
    "print(long_conv_df['class'].value_counts())\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 3. ì‹¤ì œ ëŒ€í™” ë‚´ìš© í™•ì¸ (ìƒìœ„ 5ê°œ)\n",
    "for i, row in long_conv_df.head(5).iterrows():\n",
    "    print(f\"[Index: {row['idx']} | Class: {row['class']} | Length: {row['char_len']}]\")\n",
    "    print(row['conversation'])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wct1gBUbinAo"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. ë¬¸ì¥ ë¶€í˜¸ ì¹´ìš´íŠ¸ í•¨ìˆ˜\n",
    "def count_punctuation(text):\n",
    "    return {\n",
    "        'question_mark': text.count('?'),\n",
    "        'exclamation_mark': text.count('!')\n",
    "    }\n",
    "\n",
    "# 2. ë¶„ì„ ì ìš©\n",
    "punc_counts = df['conversation'].apply(lambda x: pd.Series(count_punctuation(x)))\n",
    "df_punc = pd.concat([df['class'], punc_counts], axis=1)\n",
    "\n",
    "# 3. í´ë˜ìŠ¤ë³„ í‰ê· ì¹˜ í™•ì¸\n",
    "punc_stats = df_punc.groupby('class').mean()\n",
    "print(\"--- í´ë˜ìŠ¤ë³„ ë¬¸ì¥ ë¶€í˜¸ í‰ê·  ì‚¬ìš© íšŸìˆ˜ ---\")\n",
    "print(punc_stats)\n",
    "\n",
    "# 4. ì‹œê°í™”\n",
    "punc_stats.plot(kind='bar', figsize=(12, 6))\n",
    "plt.title('ë²”ì£„ ìœ í˜•ë³„ ë¬¸ì¥ ë¶€í˜¸ ì‚¬ìš© íŒ¨í„´')\n",
    "plt.ylabel('í‰ê·  ì‚¬ìš© íšŸìˆ˜')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qvM1TYDlll4j"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. í™•ì¸í•  ê°„íˆ¬ì–´ ë¦¬ìŠ¤íŠ¸ ì •ì˜\n",
    "filler_words = ['ì•„', 'ìŒ', 'ì–´', 'ì €ê¸°', 'ê·¸ê²Œ', 'ìˆì–ì•„', 'ë§‰']\n",
    "\n",
    "# 2. ê°„íˆ¬ì–´ ê°œìˆ˜ ê³„ì‚° í•¨ìˆ˜\n",
    "def count_fillers(text):\n",
    "    counts = {word: text.count(word) for word in filler_words}\n",
    "    counts['total_fillers'] = sum(counts.values())\n",
    "    return pd.Series(counts)\n",
    "\n",
    "# 3. ë°ì´í„°í”„ë ˆì„ì— ì ìš©\n",
    "df_fillers = df['conversation'].apply(count_fillers)\n",
    "df_analysis = pd.concat([df['class'], df_fillers], axis=1)\n",
    "\n",
    "# 4. í´ë˜ìŠ¤ë³„ í‰ê·  ê°„íˆ¬ì–´ ì‚¬ìš© íšŸìˆ˜ ìš”ì•½\n",
    "filler_stats = df_analysis.groupby('class')['total_fillers'].mean().reset_index()\n",
    "\n",
    "print(\"--- í´ë˜ìŠ¤ë³„ í‰ê·  ê°„íˆ¬ì–´ ì‚¬ìš© íšŸìˆ˜ ---\")\n",
    "print(filler_stats)\n",
    "\n",
    "# 5. ì‹œê°í™”\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=filler_stats, x='class', y='total_fillers')\n",
    "plt.title('í´ë˜ìŠ¤ë³„ ê°„íˆ¬ì–´(Filler Words) í‰ê·  ì‚¬ìš© ë¹ˆë„')\n",
    "plt.ylabel('í‰ê·  ì‚¬ìš© íšŸìˆ˜')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZCdHcMdpXGK"
   },
   "outputs": [],
   "source": [
    "# í´ë˜ìŠ¤ë³„ ê° ë‹¨ì–´ì˜ í‰ê·  ë°œìƒ íšŸìˆ˜ íˆíŠ¸ë§µ\n",
    "word_specific_stats = df_analysis.groupby('class')[filler_words].mean()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(word_specific_stats, annot=True, cmap='YlGnBu')\n",
    "plt.title('í´ë˜ìŠ¤ë³„ ê°„íˆ¬ì–´ ìƒì„¸ ë¶„í¬ (í‰ê· )')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WtD7jcHYpbJ-"
   },
   "outputs": [],
   "source": [
    "# 'total_fillers' ì»¬ëŸ¼ì€ df_analysisì— ìˆìŠµë‹ˆë‹¤.\n",
    "# ê°„íˆ¬ì–´ê°€ ë§ì€ ìˆœì„œëŒ€ë¡œ ìƒìœ„ 10ê°œë¥¼ ë½‘ì•„ë´…ë‹ˆë‹¤.\n",
    "# df_analysisì—ì„œ 'total_fillers'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ 10ê°œë¥¼ ì„ íƒí•©ë‹ˆë‹¤.\n",
    "# ì´ ë•Œ, ì›ë³¸ dfì˜ ì¸ë±ìŠ¤ì™€ ì¼ì¹˜í•˜ë¯€ë¡œ locë¥¼ ì‚¬ìš©í•˜ì—¬ dfì—ì„œ conversationì„ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "top_filler_analysis = df_analysis.nlargest(10, 'total_fillers')\n",
    "top_filler_samples = df.loc[top_filler_analysis.index, ['class', 'conversation']].copy()\n",
    "top_filler_samples['total_fillers'] = top_filler_analysis['total_fillers']\n",
    "\n",
    "for i, row in top_filler_samples.iterrows():\n",
    "    print(f\"[{row['class']}] ê°„íˆ¬ì–´ ê°œìˆ˜: {row['total_fillers']}\")\n",
    "    print(f\"ë‚´ìš©: {row['conversation']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HyeaFRM2p3ZF"
   },
   "outputs": [],
   "source": [
    "df_t = pd.read_csv(\"/content/drive/MyDrive/DLthon/test (1).csv\")\n",
    "df_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbpG1RMYI4ud"
   },
   "source": [
    "### ë² ì´ìŠ¤ë¼ì¸ ì„ ì •"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RU30T27yJhwd"
   },
   "source": [
    "### ë°ì´í„° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XB0wODkkI6r6"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/content/drive/MyDrive/DLthon/train (2).csv\")\n",
    "df_test = pd.read_csv(\"/content/drive/MyDrive/DLthon/test (1).csv\")\n",
    "data_normal = pd.read_csv(\"/content/drive/MyDrive/DLthon/normal_conversation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "THOAnHRQVfs7"
   },
   "outputs": [],
   "source": [
    "data_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ULX22NcMHUg"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "glo50y5GOJzY"
   },
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7J_vFW6sWTLy"
   },
   "outputs": [],
   "source": [
    "df_normal =  pd.read_csv(\"/content/drive/MyDrive/DLthon/normal_conversation.csv\")\n",
    "df_normal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swECW0mXUOR5"
   },
   "source": [
    "ë°ì´í„° ì „ì²˜ë¦¬ (ë¬¸ì¥ë¶€í˜¸ ì •ì œ + í™”ìë¶„ë¦¬ë¥¼ SEP tokenì¶”ê°€)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "df7KguPeS2kh"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import re\n",
    "# import os\n",
    "\n",
    "# # 1. ì¼ë°˜ ëŒ€í™” ë°ì´í„° ë¡œë“œ\n",
    "# df_normal = pd.read_csv(\"/content/drive/MyDrive/DLthon/normal_conversation.csv\")\n",
    "\n",
    "# # 2. ì „ì²˜ë¦¬ í•¨ìˆ˜ (ë¶€í˜¸ ì •ì œ + [SEP] ì‚½ì…)\n",
    "# def final_preprocessing(text):\n",
    "#     if not isinstance(text, str): return text\n",
    "#     # ë¬¸ì¥ ë¶€í˜¸ ì •ì œ\n",
    "#     text = re.sub(r'!{2,}', '!', text)\n",
    "#     text = re.sub(r'\\?{2,}', '?', text)\n",
    "#     text = re.sub(r'\\.{2,}', '.', text)\n",
    "#     # \\n -> [SEP] ì¹˜í™˜\n",
    "#     lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "#     return \" [SEP] \".join(lines)\n",
    "\n",
    "# # 3. ê° ë°ì´í„°í”„ë ˆì„ ì „ì²˜ë¦¬ ì ìš©\n",
    "# # Train (ê¸°ì¡´ ë²”ì£„ ëŒ€í™”)\n",
    "# data['conversation'] = data['conversation'].apply(final_preprocessing)\n",
    "\n",
    "# # Normal (ì¼ë°˜ ëŒ€í™”)\n",
    "# col_normal = 'conversation' if 'conversation' in df_normal.columns else 'text'\n",
    "# df_normal[col_normal] = df_normal[col_normal].apply(final_preprocessing)\n",
    "# # ë§Œì•½ ì¼ë°˜ ëŒ€í™”ì— class ì»¬ëŸ¼ì´ ì—†ë‹¤ë©´ ì¶”ê°€ (ë¼ë²¨ë§ì„ ìœ„í•´)\n",
    "# if 'class' not in df_normal.columns:\n",
    "#     df_normal['class'] = 'ì¼ë°˜ ëŒ€í™”'\n",
    "\n",
    "# # Test (í…ŒìŠ¤íŠ¸ ëŒ€í™”)\n",
    "# col_test = 'text' if 'text' in df_test.columns else 'conversation'\n",
    "# df_test[col_test] = df_test[col_test].apply(final_preprocessing)\n",
    "\n",
    "# # 4. í•™ìŠµ ë°ì´í„° í†µí•© (ë²”ì£„ + ì¼ë°˜)\n",
    "# data_combined = pd.concat([data, df_normal], ignore_index=True)\n",
    "\n",
    "# # 5. ì €ì¥ ê²½ë¡œ ì„¤ì •\n",
    "# save_path = \"/content/drive/MyDrive/DLthon/processed_data\"\n",
    "# if not os.path.exists(save_path):\n",
    "#     os.makedirs(save_path)\n",
    "\n",
    "# # 6. ìµœì¢… íŒŒì¼ ì €ì¥\n",
    "# data_combined.to_csv(f\"{save_path}/train_combined_sep.csv\", index=False, encoding='utf-8-sig')\n",
    "# df_test.to_csv(f\"{save_path}/test_cleaned_sep.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "# print(f\"âœ… í•™ìŠµ ë°ì´í„° í†µí•© ì™„ë£Œ: {len(data_combined)}ê°œ ìƒ˜í”Œ\")\n",
    "# print(f\"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ: {len(df_test)}ê°œ ìƒ˜í”Œ\")\n",
    "# print(f\"ğŸ“‚ ì €ì¥ ê²½ë¡œ: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mR5cmeqUTi1"
   },
   "source": [
    "ë°ì´í„° ì „ì²˜ë¦¬ (ë¬¸ì¥ ë¶€í˜¸ë§Œ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v51ji-skUHxy"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import re\n",
    "# import os\n",
    "\n",
    "# # 1. ì¼ë°˜ ëŒ€í™” ë°ì´í„° ë¡œë“œ\n",
    "# df_normal = pd.read_csv(\"/content/drive/MyDrive/DLthon/normal_conversation.csv\")\n",
    "\n",
    "# # 2. ì „ì²˜ë¦¬ í•¨ìˆ˜ (ë¶€í˜¸ ì •ì œ + \\nì„ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜)\n",
    "# def simplify_no_sep(text):\n",
    "#     if not isinstance(text, str): return text\n",
    "#     # ë¬¸ì¥ ë¶€í˜¸ ì •ì œ (!, ?, . í•˜ë‚˜ë§Œ ë‚¨ê¸°ê¸°)\n",
    "#     text = re.sub(r'!{2,}', '!', text)\n",
    "#     text = re.sub(r'\\?{2,}', '?', text)\n",
    "#     text = re.sub(r'\\.{2,}', '.', text)\n",
    "#     # \\nì„ [SEP] ëŒ€ì‹  'ê³µë°±'ìœ¼ë¡œ ì¹˜í™˜í•˜ì—¬ ë¬¸ì¥ì„ í•˜ë‚˜ë¡œ ì‡ê¸°\n",
    "#     processed_text = \" \".join([line.strip() for line in text.split('\\n') if line.strip()])\n",
    "#     return processed_text\n",
    "\n",
    "# # 3. ê° ë°ì´í„°í”„ë ˆì„ ë³µì‚¬ ë° ì „ì²˜ë¦¬ ì ìš©\n",
    "# # Train (ê¸°ì¡´ ë²”ì£„ ëŒ€í™”)\n",
    "# df_train_no_sep = data.copy()\n",
    "# df_train_no_sep['conversation'] = df_train_no_sep['conversation'].apply(simplify_no_sep)\n",
    "\n",
    "# # Normal (ì¼ë°˜ ëŒ€í™”)\n",
    "# df_normal_no_sep = df_normal.copy()\n",
    "# col_normal = 'conversation' if 'conversation' in df_normal_no_sep.columns else 'text'\n",
    "# df_normal_no_sep[col_normal] = df_normal_no_sep[col_normal].apply(simplify_no_sep)\n",
    "# if 'class' not in df_normal_no_sep.columns:\n",
    "#     df_normal_no_sep['class'] = 'ì¼ë°˜ ëŒ€í™”'\n",
    "\n",
    "# # Test (í…ŒìŠ¤íŠ¸ ëŒ€í™”)\n",
    "# df_test_no_sep = df_test.copy()\n",
    "# col_test = 'text' if 'text' in df_test_no_sep.columns else 'conversation'\n",
    "# df_test_no_sep[col_test] = df_test_no_sep[col_test].apply(simplify_no_sep)\n",
    "\n",
    "# # 4. í•™ìŠµ ë°ì´í„° í†µí•© (ë²”ì£„ + ì¼ë°˜)\n",
    "# train_combined_no_sep = pd.concat([df_train_no_sep, df_normal_no_sep], ignore_index=True)\n",
    "\n",
    "# # 5. ì €ì¥ ê²½ë¡œ ì„¤ì • ë° íŒŒì¼ ì €ì¥\n",
    "# save_path = \"/content/drive/MyDrive/DLthon/processed_data\"\n",
    "# if not os.path.exists(save_path):\n",
    "#     os.makedirs(save_path)\n",
    "\n",
    "# train_combined_no_sep.to_csv(f\"{save_path}/train_combined_no_sep.csv\", index=False, encoding='utf-8-sig')\n",
    "# df_test_no_sep.to_csv(f\"{save_path}/test_no_sep.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "# print(f\"âœ… [No-SEP] í†µí•© í•™ìŠµ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {len(train_combined_no_sep)}ê°œ ìƒ˜í”Œ\")\n",
    "# print(f\"âœ… [No-SEP] í…ŒìŠ¤íŠ¸ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {len(df_test_no_sep)}ê°œ ìƒ˜í”Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4H498d7Upe2"
   },
   "source": [
    "### í† í°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nKzkbsaZgmff"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/DLthon/processed_data/train_combined_sep.csv\")\n",
    "df_test = pd.read_csv(\"/content/drive/MyDrive/DLthon/processed_data/test_cleaned_sep.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ado-71tDhS_u"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDkh-k3rhK7d"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# 1. ë ˆì´ë¸” ì¸ì½”ë” ìƒì„± ë° í•™ìŠµ\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['class'])\n",
    "# í´ë˜ìŠ¤-ìˆ«ì ë§¤í•‘ ê²°ê³¼ ì €ì¥ (ë‚˜ì¤‘ì— ì¶”ë¡  ì‹œ ê²°ê³¼ í•´ì„ì„ ìœ„í•´ í•„ìš”)\n",
    "mapping = dict(zip(range(len(le.classes_)), le.classes_))\n",
    "print(\"ğŸ“Œ ë ˆì´ë¸” ë§¤í•‘ ê²°ê³¼:\", mapping)\n",
    "\n",
    "# 2. íŒŒì´í† ì¹˜ í…ì„œë¡œ ë³€í™˜\n",
    "# ì•ì„œ ë§Œë“  input_idsì™€ attention_masksë¥¼ í•¨ê»˜ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "train_labels = torch.tensor(df['label'].values)\n",
    "print(f\"ë ˆì´ë¸” í…ì„œ ìƒì„± ì™„ë£Œ: {train_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3RdnnXj_YhO7"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "\n",
    "# 1. í™˜ê²½ ì„¤ì •\n",
    "CHECKPOINT_NAME = \"klue/bert-base\"\n",
    "tokenizer_pretrained = CHECKPOINT_NAME\n",
    "\n",
    "# 2. ë°ì´í„° ë¶„í•  (Stratify ì ìš©)\n",
    "# í•™ìŠµ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„°ë¥¼ 8:2 ë¹„ìœ¨ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.\n",
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df['label']  # í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€\n",
    ")\n",
    "\n",
    "# 3. ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ (ì œê³µí•´ì£¼ì‹  ì½”ë“œ ìˆ˜ì •)\n",
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer_pretrained):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_pretrained)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # ì»¬ëŸ¼ëª…ì— ë§ì¶° ìˆ˜ì •: 'document' -> 'conversation'\n",
    "        sentence = self.data.iloc[idx]['conversation']\n",
    "        label = self.data.iloc[idx]['label']\n",
    "\n",
    "        tokens = self.tokenizer(\n",
    "            str(sentence),           # ë¬¸ì¥ ì „ë‹¬\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=512,          # ê¸¸ì´ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "\n",
    "        input_ids = tokens['input_ids'].squeeze(0)\n",
    "        attention_mask = tokens['attention_mask'].squeeze(0)\n",
    "        token_type_ids = torch.zeros_like(attention_mask)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'token_type_ids': token_type_ids,\n",
    "        }, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# 4. Dataset ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "train_dataset = TokenDataset(train_df, tokenizer_pretrained)\n",
    "val_dataset = TokenDataset(val_df, tokenizer_pretrained)\n",
    "\n",
    "# 5. DataLoader ì„¤ì •\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"âœ… DataLoader êµ¬ì¶• ì™„ë£Œ (Stratify ì ìš©)\")\n",
    "print(f\"í•™ìŠµ ë°ì´í„° ê°œìˆ˜: {len(train_df)} | ê²€ì¦ ë°ì´í„° ê°œìˆ˜: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89eXBPMRarMZ"
   },
   "source": [
    "### í† í°í™” í™•ì¸ ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "huPBqypHafSE"
   },
   "outputs": [],
   "source": [
    "# --- ê¸°ì¡´ ì½”ë“œ ì•„ë˜ì— ì¶”ê°€ ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT_NAME)\n",
    "\n",
    "# 1. train_loaderì—ì„œ ì²« ë²ˆì§¸ ë°°ì¹˜ë¥¼ êº¼ë‚´ì˜¤ê¸°\n",
    "batch_inputs, batch_labels = next(iter(train_loader))\n",
    "\n",
    "# 2. ì²« ë²ˆì§¸ ìƒ˜í”Œ ë°ì´í„° ì¶”ì¶œ\n",
    "# batch_inputsëŠ” {'input_ids': ..., 'attention_mask': ..., 'token_type_ids': ...} í˜•íƒœì…ë‹ˆë‹¤.\n",
    "sample_input_ids = batch_inputs['input_ids'][0]\n",
    "sample_attention_mask = batch_inputs['attention_mask'][0]\n",
    "sample_label = batch_labels[0]\n",
    "\n",
    "# 3. í† í° IDë¥¼ ì‹¤ì œ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”© (ë³µì›)\n",
    "# skip_special_tokens=Falseë¡œ ì„¤ì •í•˜ì—¬ [CLS], [SEP], [PAD]ê°€ ì–´ë””ì— ìœ„ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "decoded_sentence = tokenizer.decode(sample_input_ids, skip_special_tokens=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ğŸ” [DataLoader ìƒ˜í”Œ ë°ì´í„° í™•ì¸]\")\n",
    "print(\"=\"*50)\n",
    "print(f\"ğŸ“Œ ì‹¤ì œ ë ˆì´ë¸” (ìˆ«ì): {sample_label.item()} (í´ë˜ìŠ¤: {le.inverse_transform([sample_label.item()])[0]})\")\n",
    "print(f\"ğŸ“Œ Input IDs í˜•íƒœ: {sample_input_ids.shape}\")\n",
    "print(\"-\" * 50)\n",
    "print(\"ğŸ“Œ í† í°í™” ë° ë³µì›ëœ ë¬¸ì¥ (ì•ë¶€ë¶„ 300ì):\")\n",
    "print(decoded_sentence[:300] + \"...\")\n",
    "print(\"-\" * 50)\n",
    "print(\"ğŸ“Œ ì‹¤ì œ í† í° ID ë¦¬ìŠ¤íŠ¸ (ì•ë¶€ë¶„ 20ê°œ):\")\n",
    "print(sample_input_ids[:20].tolist())\n",
    "print(\"-\" * 50)\n",
    "print(\"ğŸ“Œ Attention Mask (ì•ë¶€ë¶„ 20ê°œ):\")\n",
    "print(sample_attention_mask[:20].tolist())\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-xDe9uGaveN"
   },
   "source": [
    "### BERT (ì´ë¯¸ í•™ìŠµë˜ì–´ ìˆëŠ”ê±° fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vOVPp3QZbyIM"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z08JtLoah4nU"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "class CustomBertModel(nn.Module):\n",
    "    def __init__(self, bert_pretrained, dropout_rate=0.5):\n",
    "        super(CustomBertModel, self).__init__()\n",
    "        # ì‚¬ì „í•™ìŠµ ëª¨ë¸ ì§€ì •\n",
    "        self.bert = BertModel.from_pretrained(bert_pretrained)\n",
    "        self.dr = nn.Dropout(p=dropout_rate)\n",
    "        # 5 class ë¶„ë¥˜\n",
    "        self.fc = nn.Linear(768, 5)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        last_hidden_state = output['last_hidden_state']\n",
    "        # last_hidden_state[:, 0, :]ëŠ” [CLS] í† í°ì„ ê°€ì ¸ì˜´\n",
    "        x = self.dr(last_hidden_state[:, 0, :])\n",
    "        # FC ì„ ê±°ì³ ìµœì¢… ì¶œë ¥\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mW4ZaKWSjcov"
   },
   "outputs": [],
   "source": [
    "bert = CustomBertModel(CHECKPOINT_NAME)\n",
    "bert.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l5nMtP-ciS6y"
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(bert.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SL7fz1m9in5a"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def model_train(model, data_loader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    corr = 0\n",
    "    counts = 0\n",
    "\n",
    "    # F1 Score ê³„ì‚°ì„ ìœ„í•´ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    progress_bar = tqdm(data_loader, unit='batch', total=len(data_loader), mininterval=1)\n",
    "\n",
    "    for idx, (inputs, labels) in enumerate(progress_bar):\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # ëª¨ë¸ ì¶œë ¥ ë° ë¡œì§“(logits) ì¶”ì¶œ\n",
    "        output = model(**inputs)\n",
    "        logits = output.logits if hasattr(output, 'logits') else output\n",
    "\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, pred = logits.max(dim=1)\n",
    "\n",
    "        # í†µê³„ì¹˜ ê³„ì‚°\n",
    "        corr += pred.eq(labels).sum().item()\n",
    "        counts += len(labels)\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "\n",
    "        # F1 Scoreìš© ë°ì´í„° ìˆ˜ì§‘\n",
    "        all_preds.extend(pred.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # ì§„í–‰ ë°” ì—…ë°ì´íŠ¸ (í˜„ì¬ê¹Œì§€ì˜ F1 í¬í•¨)\n",
    "        current_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        progress_bar.set_description(f\"Loss: {running_loss/counts:.4f}, Acc: {corr/counts:.4f}, F1: {current_f1:.4f}\")\n",
    "\n",
    "    final_loss = running_loss / len(data_loader.dataset)\n",
    "    final_acc = corr / len(data_loader.dataset)\n",
    "    final_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    return final_loss, final_acc, final_f1\n",
    "\n",
    "def model_evaluate(model, data_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0\n",
    "    corr = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            output = model(**inputs)\n",
    "            logits = output.logits if hasattr(output, 'logits') else output\n",
    "\n",
    "            _, pred = logits.max(dim=1)\n",
    "\n",
    "            corr += torch.sum(pred.eq(labels)).item()\n",
    "            running_loss += loss_fn(logits, labels).item() * labels.size(0)\n",
    "\n",
    "            # F1 Scoreìš© ë°ì´í„° ìˆ˜ì§‘\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    final_loss = running_loss / len(data_loader.dataset)\n",
    "    final_acc = corr / len(data_loader.dataset)\n",
    "    final_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    return final_loss, final_acc, final_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gLnWgq_nish9"
   },
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "model_name = 'bert-kor-base'\n",
    "min_loss = np.inf\n",
    "\n",
    "metric_path = f\"{model_name}_metrics.txt\"\n",
    "\n",
    "history = {'train_loss': [], 'train_f1': [], 'val_loss': [], 'val_f1': []}\n",
    "\n",
    "# ğŸ”¹ íŒŒì¼ í—¤ë” (ì²˜ìŒ í•œ ë²ˆë§Œ)\n",
    "with open(metric_path, 'w') as f:\n",
    "    f.write(\"Epoch\\tTrain_Loss\\tTrain_Acc\\tTrain_F1\\tVal_Loss\\tVal_Acc\\tVal_F1\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    train_loss, train_acc, train_f1 = model_train(\n",
    "        bert, train_loader, loss_fn, optimizer, device\n",
    "    )\n",
    "\n",
    "    # Evaluation\n",
    "    val_loss, val_acc, val_f1 = model_evaluate(\n",
    "        bert, val_loader, loss_fn, device\n",
    "    )\n",
    "\n",
    "    # Checkpoint\n",
    "    if val_loss < min_loss:\n",
    "        print(f'âœ¨ [INFO] val_loss improved from {min_loss:.5f} to {val_loss:.5f}. Saving Model!')\n",
    "        min_loss = val_loss\n",
    "        torch.save(bert.state_dict(), f'{model_name}.pth')\n",
    "\n",
    "    # ì½˜ì†” ì¶œë ¥\n",
    "    print(f'Epoch [{epoch+1:02d}/{num_epochs}]')\n",
    "    print(f'TRAIN | Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | F1: {train_f1:.4f}')\n",
    "    print(f'VALID | Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | F1: {val_f1:.4f}')\n",
    "    print('-' * 50)\n",
    "\n",
    "    # ğŸ”¹ TXT íŒŒì¼ì— metric ì €ì¥\n",
    "    with open(metric_path, 'a') as f:\n",
    "        f.write(\n",
    "            f\"{epoch+1}\\t\"\n",
    "            f\"{train_loss:.4f}\\t{train_acc:.4f}\\t{train_f1:.4f}\\t\"\n",
    "            f\"{val_loss:.4f}\\t{val_acc:.4f}\\t{val_f1:.4f}\\n\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p34gb6-SdbQJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. ìµœì ì˜ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "bert.load_state_dict(torch.load(f'{model_name}.pth'))\n",
    "bert.to(device)\n",
    "bert.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# 2. ì¶”ë¡  ì§„í–‰\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = bert(input_ids, attention_mask, token_type_ids)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 3. ë¦¬í¬íŠ¸ ì¶œë ¥\n",
    "target_names = ['í˜‘ë°•', 'ê°ˆì·¨', 'ì§ì¥ ë‚´ ê´´ë¡­í˜', 'ê¸°íƒ€ ê´´ë¡­í˜', 'ì¼ë°˜ ëŒ€í™”']\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" [ìµœì¢… ëª¨ë¸ í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¦¬í¬íŠ¸] \")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(all_labels, all_preds, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
